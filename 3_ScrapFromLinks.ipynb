{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup  # HTML data structure\n",
    "from urllib.request import urlretrieve, urlopen as uReq  # Web client\n",
    "from time import time\n",
    "import os\n",
    "from multiprocessing import Process, Pool # SubProcessing\n",
    "import shutil # directory manipulation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveImages(car_id, car_soup):\n",
    "    path = 'images/{}'.format(car_id)\n",
    "    \n",
    "    # create specific folder\n",
    "    if os.path.isdir(path):\n",
    "        shutil.rmtree(path)\n",
    "    os.mkdir(path)\n",
    "    \n",
    "    # image processing\n",
    "    images = car_soup.findAll('div', {'class': 'thumbnail-image'})\n",
    "    for ind, image in enumerate(images):\n",
    "        img_src = image.find('img')['src']\n",
    "        urlretrieve(img_src, \"{}/{}.jpg\".format(path, ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCSV(car_url, car_data_dict, car_soup):\n",
    "    # FEATURE: car ID\n",
    "    car_id = car_url.split('/')[5]\n",
    "    car_data_dict['ID'] = car_id\n",
    "\n",
    "    # FEATURE: car Price\n",
    "    gel_price, usd_price = car_soup.findAll('span', {'class': 'car-price'})\n",
    "    usd_price = usd_price.text\n",
    "    if usd_price != 'Price negotiable':\n",
    "        usd_price = usd_price.split()[0]\n",
    "        usd_price = usd_price.replace(',', '')\n",
    "        car_data_dict['Price'] = usd_price\n",
    "    else:\n",
    "        car_data_dict['Price'] = 'Price negotiable'\n",
    "\n",
    "    # FEATURE: car customs\n",
    "    custom_soup = car_soup.find('div', {'class': 'levy'})\n",
    "    if custom_soup:\n",
    "        custom = custom_soup.findAll('span')[1].findAll('span')[1].text\n",
    "        custom = custom.split()[0]\n",
    "        custom = custom.replace(',', '')\n",
    "        car_data_dict['Customs'] = custom\n",
    "    else:\n",
    "        car_data_dict['Customs'] = ''\n",
    "\n",
    "    # FEATURE: Data from <tr>-s\n",
    "    car_data_table = car_soup.findAll('tr')\n",
    "    for data in car_data_table:\n",
    "        th1, th2 = data.findAll('th')\n",
    "\n",
    "        # th1\n",
    "        th1_divs = th1.findAll('div')\n",
    "        if len(th1_divs) == 2:\n",
    "            th1_key, th1_value = map(\n",
    "                lambda x: x.text.strip(), th1_divs)\n",
    "            if th1_key in car_data_dict:\n",
    "                car_data_dict[th1_key] = th1_value\n",
    "\n",
    "        # th2\n",
    "        th2_divs = th2.findAll('div')\n",
    "        if len(th2_divs) == 2:\n",
    "            th2_key, th2_value = th2_divs\n",
    "            th2_key = th2_key.text.strip()\n",
    "            if 'class' in th2_value.i.attrs and 'fa-check' in th2_value.i['class']:\n",
    "                res = '1'\n",
    "            else:\n",
    "                res = '0'\n",
    "\n",
    "            th2_value = res\n",
    "            if th2_key in car_data_dict:\n",
    "                car_data_dict[th2_key] = th2_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log(ind, start, total, PID):\n",
    "    now = time()\n",
    "    ind += 1\n",
    "    elapsed = int(now-start)\n",
    "    if elapsed == 0:\n",
    "        elapsed = 1 \n",
    "    mean = (elapsed/ind)\n",
    "    estimated = (total-ind)//mean\n",
    "    print('Thread {}: Cars {}/{} -- Time Elapsed {}s -- Time Left {}s'.format(PID, ind, total, elapsed, estimated))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapLink(data,):\n",
    "    start = time()\n",
    "    \n",
    "    # data from argument\n",
    "    PID, links, headers = data\n",
    "    \n",
    "    result = []\n",
    "    for ind, car_url in enumerate(links):\n",
    "        _log(ind, start, len(links), PID)\n",
    "        try:\n",
    "            # go to each car's url\n",
    "            uClient = uReq(car_url)\n",
    "            car_soup = soup(uClient.read(), \"html.parser\")\n",
    "            uClient.close()\n",
    "\n",
    "            # DATA\n",
    "            car_data_dict = {header: '' for header in headers}\n",
    "\n",
    "            saveCSV(car_url, car_data_dict, car_soup)\n",
    "            saveImages(car_data_dict['ID'], car_soup)\n",
    "            \n",
    "            result.append(car_data_dict)\n",
    "            if (ind > 10):\n",
    "                return result\n",
    "        except:\n",
    "            print(\"Error while processing.. {}\".format(ind))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read links\n",
    "linksFile = open('SabaLinks.txt', 'r')\n",
    "links = [line.rstrip() for line in linksFile]\n",
    "linksFile.close()\n",
    "\n",
    "# open folder for writing result\n",
    "folder = 'data'\n",
    "if os.path.isdir(folder):\n",
    "    shutil.rmtree(folder)\n",
    "os.mkdir(folder)\n",
    "os.chdir(folder)\n",
    "\n",
    "# create images folder\n",
    "os.mkdir('images')\n",
    "\n",
    "headers = ['ID', 'Manufacturer', 'Model', 'Category', 'Mileage', 'Gear box type', 'Doors',\n",
    "                'Wheel', 'Color', 'Interior color', 'VIN', 'Leather interior', 'Price', 'Customs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total processes\n",
    "procces_count = 10\n",
    "\n",
    "# data for each process\n",
    "ProcessDataArray = []\n",
    "\n",
    "# links count for each process\n",
    "chunks_size = len(links) // procces_count\n",
    "    \n",
    "# divide data into chunks\n",
    "for i in range(procces_count): \n",
    "    chunk = links[chunks_size * i: chunks_size * (i + 1)]\n",
    "    if i == procces_count - 1:\n",
    "        chunk = links[chunks_size * i:]\n",
    "    # data for one process\n",
    "    processData = (i, chunk, headers)\n",
    "    \n",
    "    ProcessDataArray.append(processData) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create pool of processes\n",
    "p = Pool(procces_count)\n",
    "\n",
    "# map each process to function \"scraplink\" with and itterate over argument ProcessDataArray\n",
    "results = p.map(scrapLink, ProcessDataArray)\n",
    "\n",
    "# wait for all processes to stop working\n",
    "p.terminate()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all data into one dataframe\n",
    "final_result = pd.DataFrame(columns = headers)\n",
    "for result in results:\n",
    "    final_result = final_result.append(pd.DataFrame(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write features dataframe into csv\n",
    "file_name = 'cars.csv'\n",
    "final_result.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
